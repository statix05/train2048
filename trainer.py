"""
–°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è 2048
–í–∫–ª—é—á–∞–µ—Ç:
- –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
- –í–∞–ª–∏–¥–∞—Ü–∏—é –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
"""
import numpy as np
import time
import os
from typing import List, Tuple, Optional
from collections import deque
import json
from datetime import datetime

from game_2048 import Game2048, Direction
from neural_network import DQNAgent, device


class Trainer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –∏–≥—Ä–∞—Ç—å –≤ 2048
    """
    
    def __init__(
        self,
        agent: DQNAgent,
        save_dir: str = "models",
        log_dir: str = "logs"
    ):
        self.agent = agent
        self.save_dir = save_dir
        self.log_dir = log_dir
        
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.episode_rewards = []
        self.episode_scores = []
        self.episode_max_tiles = []
        self.episode_moves = []
        self.training_losses = []
        
        self.best_score = 0
        self.best_max_tile = 0
        
        # Moving averages –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.reward_window = deque(maxlen=100)
        self.score_window = deque(maxlen=100)
        self.tile_window = deque(maxlen=100)
    
    def train_episode(self, game: Game2048, render: bool = False) -> dict:
        """
        –û–¥–∏–Ω —ç–ø–∏–∑–æ–¥ –æ–±—É—á–µ–Ω–∏—è
        """
        state = game.get_state()
        features = game.get_features()
        
        total_reward = 0.0
        episode_loss = []
        
        while True:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Ö–æ–¥—ã
            valid_moves = game.get_valid_moves()
            if not valid_moves:
                break
            
            valid_moves_int = [int(m) for m in valid_moves]
            
            # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            action = self.agent.select_action(state, features, valid_moves_int)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            reward, done, info = game.move(Direction(action))
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            next_state = game.get_state()
            next_features = game.get_features()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç
            self.agent.store_experience(
                state, features, action, reward,
                next_state, next_features, done
            )
            
            # –û–±—É—á–∞–µ–º
            loss = self.agent.train_step()
            if loss is not None:
                episode_loss.append(loss)
            
            total_reward += reward
            state = next_state
            features = next_features
            
            if done:
                break
            
            if render:
                print(game)
                print(f"Action: {Direction(action).name}, Reward: {reward:.2f}")
                time.sleep(0.1)
        
        return {
            'reward': total_reward,
            'score': game.score,
            'max_tile': game.max_tile,
            'moves': game.moves,
            'loss': np.mean(episode_loss) if episode_loss else 0.0
        }
    
    def train(
        self,
        n_episodes: int = 10000,
        save_every: int = 500,
        eval_every: int = 100,
        eval_episodes: int = 10,
        verbose: bool = True
    ):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
        """
        start_time = time.time()
        
        for episode in range(1, n_episodes + 1):
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∏–≥—Ä—É
            game = Game2048()
            
            # –û–±—É—á–∞–µ–º —ç–ø–∏–∑–æ–¥
            result = self.train_episode(game)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            self.episode_rewards.append(result['reward'])
            self.episode_scores.append(result['score'])
            self.episode_max_tiles.append(result['max_tile'])
            self.episode_moves.append(result['moves'])
            if result['loss'] > 0:
                self.training_losses.append(result['loss'])
            
            self.reward_window.append(result['reward'])
            self.score_window.append(result['score'])
            self.tile_window.append(result['max_tile'])
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if result['score'] > self.best_score:
                self.best_score = result['score']
            if result['max_tile'] > self.best_max_tile:
                self.best_max_tile = result['max_tile']
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if verbose and episode % 10 == 0:
                avg_reward = np.mean(self.reward_window)
                avg_score = np.mean(self.score_window)
                avg_tile = np.mean(self.tile_window)
                epsilon = self.agent.get_epsilon()
                
                elapsed = time.time() - start_time
                eps_per_sec = episode / elapsed
                
                print(f"Episode {episode}/{n_episodes} | "
                      f"Avg Reward: {avg_reward:.1f} | "
                      f"Avg Score: {avg_score:.0f} | "
                      f"Avg Max Tile: {avg_tile:.0f} | "
                      f"Best: {self.best_score} ({self.best_max_tile}) | "
                      f"Œµ: {epsilon:.3f} | "
                      f"Speed: {eps_per_sec:.1f} ep/s")
            
            # –û—Ü–µ–Ω–∫–∞
            if episode % eval_every == 0:
                eval_result = self.evaluate(eval_episodes)
                print(f"\nüìä Evaluation ({eval_episodes} games):")
                print(f"   Score: {eval_result['avg_score']:.0f} ¬± {eval_result['std_score']:.0f}")
                print(f"   Max Tile Avg: {eval_result['avg_max_tile']:.0f}")
                print(f"   Best in Eval: {eval_result['best_score']} ({eval_result['best_max_tile']})")
                
                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –ø–ª–∏—Ç–æ–∫
                tile_dist = eval_result['tile_distribution']
                dist_str = ", ".join([f"{k}: {v}" for k, v in sorted(tile_dist.items(), key=lambda x: -x[1])])
                print(f"   Tile Distribution: {dist_str}\n")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            if episode % save_every == 0:
                self.save_checkpoint(episode)
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_checkpoint(n_episodes, final=True)
        self.save_logs()
        
        print(f"\nüéâ Training completed!")
        print(f"   Total episodes: {n_episodes}")
        print(f"   Best score: {self.best_score}")
        print(f"   Best max tile: {self.best_max_tile}")
        print(f"   Total time: {time.time() - start_time:.1f}s")
    
    def evaluate(self, n_episodes: int = 10) -> dict:
        """
        –û—Ü–µ–Ω–∫–∞ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏ (–±–µ–∑ exploration)
        """
        scores = []
        max_tiles = []
        moves_list = []
        
        for _ in range(n_episodes):
            game = Game2048()
            state = game.get_state()
            features = game.get_features()
            
            while True:
                valid_moves = game.get_valid_moves()
                if not valid_moves:
                    break
                
                valid_moves_int = [int(m) for m in valid_moves]
                
                # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –±–µ–∑ exploration (epsilon=0)
                action = self.agent.policy_net.get_action(
                    state, features, valid_moves_int, epsilon=0.0
                )
                
                _, done, _ = game.move(Direction(action))
                
                state = game.get_state()
                features = game.get_features()
                
                if done:
                    break
            
            scores.append(game.score)
            max_tiles.append(game.max_tile)
            moves_list.append(game.moves)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –ø–ª–∏—Ç–æ–∫
        tile_dist = {}
        for tile in max_tiles:
            tile_dist[tile] = tile_dist.get(tile, 0) + 1
        
        return {
            'avg_score': np.mean(scores),
            'std_score': np.std(scores),
            'avg_max_tile': np.mean(max_tiles),
            'best_score': max(scores),
            'best_max_tile': max(max_tiles),
            'avg_moves': np.mean(moves_list),
            'tile_distribution': tile_dist
        }
    
    def save_checkpoint(self, episode: int, final: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏"""
        filename = "model_final.pt" if final else f"model_ep{episode}.pt"
        path = os.path.join(self.save_dir, filename)
        self.agent.save(path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if self.episode_scores and self.episode_scores[-1] >= self.best_score * 0.95:
            best_path = os.path.join(self.save_dir, "model_best.pt")
            self.agent.save(best_path)
    
    def save_logs(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        log_data = {
            'episode_rewards': self.episode_rewards,
            'episode_scores': self.episode_scores,
            'episode_max_tiles': self.episode_max_tiles,
            'episode_moves': self.episode_moves,
            'training_losses': self.training_losses,
            'best_score': self.best_score,
            'best_max_tile': self.best_max_tile,
            'timestamp': datetime.now().isoformat()
        }
        
        log_path = os.path.join(self.log_dir, "training_log.json")
        with open(log_path, 'w') as f:
            json.dump(log_data, f)
        
        print(f"Logs saved to {log_path}")


def quick_train(episodes: int = 1000):
    """–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print(f"üöÄ Starting quick training on {device}")
    print(f"   Episodes: {episodes}")
    print("-" * 50)
    
    agent = DQNAgent(
        learning_rate=5e-4,
        buffer_size=50000,
        batch_size=64,
        target_update=500,
        epsilon_decay=episodes * 5
    )
    
    trainer = Trainer(agent)
    trainer.train(
        n_episodes=episodes,
        save_every=episodes // 2,
        eval_every=episodes // 10,
        eval_episodes=5
    )
    
    return agent, trainer


if __name__ == "__main__":
    # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    agent, trainer = quick_train(500)
